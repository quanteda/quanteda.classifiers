% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textmodel_cnnlstmemb.R
\name{textmodel_cnnlstmemb}
\alias{textmodel_cnnlstmemb}
\title{[Experimental] Convolutional NN + LSTM model fitted to word embeddings}
\usage{
textmodel_cnnlstmemb(
  x,
  y,
  dropout = 0.2,
  filter = 48,
  kernel_size = 5,
  pool_size = 4,
  units_lstm = 128,
  words = NULL,
  maxsenlen = NULL,
  wordembeddim = 30,
  cnnlayer = TRUE,
  fitted_embeddings = NULL,
  trainable = NULL,
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "categorical_accuracy",
  ...
)
}
\arguments{
\item{x}{tokens object}

\item{y}{vector of training labels associated with each document identified
in \code{train}.  (These will be converted to factors if not already
factors.)}

\item{dropout}{A floating variable bound between 0 and 1. It determines the
rate at which units are dropped for the linear transformation of the
inputs for the embedding layer.}

\item{filter}{The number of output filters in the convolution}

\item{kernel_size}{An integer or list of a single integer, specifying the
length of the 1D convolution window}

\item{pool_size}{Size of the max pooling windows.
\code{\link[keras:layer_max_pooling_1d]{keras::layer_max_pooling_1d()}}}

\item{units_lstm}{The number of nodes of the lstm layer}

\item{words}{The maximum number of words used to train model. Defaults to the
number of features in \code{x}}

\item{maxsenlen}{The maximum sentence length of training data}

\item{wordembeddim}{The number of word embedding dimensions to be fit}

\item{cnnlayer}{A logical parameter that allows user to include or exclude a
convolutional layer in the neural network model'}

\item{fitted_embeddings}{A fitted embeddings model formatted such that the
columns include a word identifier and embedding dimensions.The rows should
represent individual tokens.}

\item{optimizer}{optimizer used to fit model to training data, see
\code{\link[keras:compile.keras.engine.training.Model]{keras::compile.keras.engine.training.Model()}}}

\item{loss}{objective loss function, see
\code{\link[keras:compile.keras.engine.training.Model]{keras::compile.keras.engine.training.Model()}}}

\item{metrics}{metric used to train algorithm, see
\code{\link[keras:compile.keras.engine.training.Model]{keras::compile.keras.engine.training.Model()}}}

\item{...}{additional options passed to
\code{\link[keras:fit.keras.engine.training.Model]{keras::fit.keras.engine.training.Model()}}}
}
\description{
A function that combines a convolutional neural network layer with a long
short-term memory layer. It is designed to incorporate word sequences,
represented as sequentially ordered word embeddings, into text
classification. The model takes as an input a \pkg{quanteda} tokens object.
}
\examples{
\dontrun{
# create dataset with evenly balanced coded & uncoded immigration sentences
corpcoded <- corpus_subset(data_corpus_manifestosentsUK,
                           !is.na(crowd_immigration_label))
corpuncoded <- data_corpus_manifestosentsUK \%>\%
    corpus_subset(is.na(crowd_immigration_label) & year > 1980) \%>\%
    corpus_sample(size = ndoc(corpcoded))
corp <- corpcoded + corpuncoded

tok <- tokens(corp)

tmod <- textmodel_cnnlstmemb(tok,
                             y = docvars(tok, "crowd_immigration_label"),
                             epochs = 5, verbose = 1)

newdata = tokens_subset(tok, subset = is.na(crowd_immigration_label))
pred <- predict(tmod, newdata = newdata)
table(pred)
tail(texts(corpuncoded)[pred == "Immigration"], 10)

}
}
\seealso{
\code{\link[=save.textmodel_cnnlstmemb]{save.textmodel_cnnlstmemb()}}, \code{\link[=load.textmodel_cnnlstmemb]{load.textmodel_cnnlstmemb()}}
}
\keyword{textmodel}
